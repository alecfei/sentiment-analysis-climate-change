{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06b9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b088732",
   "metadata": {},
   "source": [
    "- **Install pymongo library to connect to mongodb and upload data into the database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74206abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection settings\n",
    "mongo_url = \"mongodb://localhost:27017/\"\n",
    "database_name = \"twitter_data\"\n",
    "collection_name = \"tweets\"\n",
    "\n",
    "# Define the condition to stop the scraping process\n",
    "stop_condition = 1000000  # Stop after collecting 1 million tweets\n",
    "\n",
    "# Function to scrape tweets based on a given query and save to MongoDB\n",
    "def scrape_tweets_and_save_to_mongodb(query):\n",
    "    # Initialize the MongoDB client\n",
    "    client = MongoClient(mongo_url)\n",
    "    db = client[database_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    tweet_count = 0  # Counter for the number of tweets collected\n",
    "\n",
    "    while tweet_count < stop_condition:\n",
    "        try:\n",
    "            # Scrape tweets\n",
    "            scraper = sntwitter.TwitterSearchScraper(query)\n",
    "            for i, tweet in enumerate(scraper.get_items()):\n",
    "                tweet_data = {\n",
    "                    \"date\": tweet.date,\n",
    "                    \"username\": tweet.user.username,\n",
    "                    \"user_location\": tweet.user.location,\n",
    "                    \"tweet\": tweet.content,\n",
    "                    \"num_of_like\": tweet.likeCount,\n",
    "                    \"num_of_retweet\": tweet.retweetCount,\n",
    "                    \"num_of_followers\": tweet.user.followersCount,\n",
    "                    \"num_of_reply\": tweet.replyCount\n",
    "                }\n",
    "                collection.insert_one(tweet_data)\n",
    "                tweet_count += 1\n",
    "\n",
    "                # Check if it's time to save the data\n",
    "                if tweet_count % save_interval == 0:\n",
    "                    print(f\"Data saved. Total tweets: {tweet_count}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}. Retrying in 5 seconds...\")\n",
    "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
    "\n",
    "    client.close()\n",
    "\n",
    "# Define your query\n",
    "query = \"(#weather OR 'weather') lang:en since:2022-05-01 until:2023-05-01\"\n",
    "\n",
    "# Call the function to scrape tweets and save to MongoDB\n",
    "scrape_tweets_and_save_to_mongodb(query)\n",
    "\n",
    "print(\"Data saved to MongoDB.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

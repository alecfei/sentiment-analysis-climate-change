{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bb2eb4",
   "metadata": {},
   "source": [
    "## Collect tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de044bb",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 12pt;\">This notebook demonstrates the method utilised to gather tweets on the topic of **global warming** in the past year.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa35365",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cedfee",
   "metadata": {},
   "source": [
    "- **Install snscrape library to help collecting tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d08ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed006650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import csv\n",
    "import time\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape tweets based on a given query\n",
    "def scrape_tweets(query):\n",
    "    # Initialize the list to store the scraped data\n",
    "    tweets = []\n",
    "    tweet_count = 0  # Counter for the number of tweets collected\n",
    "    \n",
    "    # Define the interval for saving the data\n",
    "    save_interval = 100  # Save every 100 tweets\n",
    "    \n",
    "    # Define the condition to stop the scraping process\n",
    "    max_tweets = 60000  # Stop after collecting 60000 tweets\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Scrape tweets\n",
    "            scraper = sntwitter.TwitterSearchScraper(query)\n",
    "            for i, tweet in enumerate(scraper.get_items()):\n",
    "                tweets.append([tweet.date, tweet.user.username, tweet.user.location, \n",
    "                               tweet.rawContent, tweet.likeCount, tweet.retweetCount, \n",
    "                               tweet.user.followersCount, tweet.replyCount])\n",
    "                tweet_count += 1\n",
    "\n",
    "                # Check if it's time to save the data\n",
    "                if tweet_count % save_interval == 0:\n",
    "                    save_data(tweets)\n",
    "                    print(f\"Data saved. Total tweets: {tweet_count}\")\n",
    "                \n",
    "                # Check if reached the maximum number of tweets\n",
    "                if tweet_count >= max_tweets:\n",
    "                    break\n",
    "                    \n",
    "            if tweet_count >= max_tweets:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}. Retrying in 5 seconds...\")\n",
    "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
    "\n",
    "    return tweets\n",
    "\n",
    "# Function to save the data to a CSV file\n",
    "def save_data(data):\n",
    "    with open(file_path, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"date\", \"username\", \"user_location\", \"tweet\", \"num_of_like\", \"num_of_retweet\",\n",
    "                         \"num_of_followers\", \"num_of_reply\"])  # Write the header\n",
    "        writer.writerows(data)  # Write the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92512bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the file path for saving the data\n",
    "# file_path = \"global warming tweets_1.csv\"\n",
    "\n",
    "# # Define query\n",
    "# query1 = \"(#globalwarming OR 'global warming') lang:en -filter:retweets since:2023-04-01 until:2023-05-01\"\n",
    "\n",
    "# # Call the function to scrape tweets\n",
    "# tweets1 = scrape_tweets(query1)\n",
    "\n",
    "# # Save the final scraped data\n",
    "# save_data(tweets1)\n",
    "# print(\"Data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8bd625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas library to briefly check the raw data. Same as below.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c10b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"global warming tweets_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307daa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67cf64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.drop_duplicates(keep = \"first\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed75c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the file path for saving the data\n",
    "# file_path = \"global warming tweets_2.csv\"\n",
    "\n",
    "# # Define query\n",
    "# query2 = \"(#globalwarming OR 'global warming') lang:en -filter:retweets since:2023-03-01 until:2023-04-01\"\n",
    "\n",
    "# # Call the function to scrape tweets\n",
    "# tweets2 = scrape_tweets(query2)\n",
    "\n",
    "# # Save the final scraped data\n",
    "# save_data(tweets2)\n",
    "# print(\"Data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf69f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"global warming tweets_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e6ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f282aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the file path for saving the data\n",
    "# file_path = \"global warming tweets_3.csv\"\n",
    "\n",
    "# # Define query\n",
    "# query3 = \"(#globalwarming OR 'global warming') lang:en -filter:retweets since:2023-02-01 until:2023-03-01\"\n",
    "\n",
    "# # Call the function to scrape tweets\n",
    "# tweets3 = scrape_tweets(query3)\n",
    "\n",
    "# # Save the final scraped data\n",
    "# save_data(tweets3)\n",
    "# print(\"Data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61370cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"global warming tweets_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee3b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d4780",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d6f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the file path for saving the data\n",
    "# file_path = \"global warming tweets_4.csv\"\n",
    "\n",
    "# # Define query\n",
    "# query4 = \"(#globalwarming OR 'global warming') lang:en -filter:retweets since:2023-01-01 until:2023-02-01\"\n",
    "\n",
    "# # Call the function to scrape tweets\n",
    "# tweets4 = scrape_tweets(query4)\n",
    "\n",
    "# # Save the final scraped data\n",
    "# save_data(tweets4)\n",
    "# print(\"Data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv(\"global warming tweets_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3630a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the file path for saving the data\n",
    "# file_path = \"global warming tweets_5.csv\"\n",
    "\n",
    "# # Define query\n",
    "# query5 = \"(#globalwarming OR 'global warming') lang:en -filter:retweets since:2022-12-01 until:2023-01-01\"\n",
    "\n",
    "# # Call the function to scrape tweets\n",
    "# tweets5 = scrape_tweets(query5)\n",
    "\n",
    "# # Save the final scraped data\n",
    "# save_data(tweets5)\n",
    "# print(\"Data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_csv(\"global warming tweets_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13340f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2bfb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f258ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the file path for saving the data\n",
    "# file_path = \"global warming tweets_6.csv\"\n",
    "\n",
    "# # Define query\n",
    "# query6 = \"(#globalwarming OR 'global warming') lang:en -filter:retweets since:2022-11-01 until:2023-12-01\"\n",
    "\n",
    "# # Call the function to scrape tweets\n",
    "# tweets6 = scrape_tweets(query6)\n",
    "\n",
    "# # Save the final scraped data\n",
    "# save_data(tweets6)\n",
    "# print(\"Data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.read_csv(\"global warming tweets_6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caef1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b61a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b7e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the file path for saving the data\n",
    "# file_path = \"global warming tweets_7.csv\"\n",
    "\n",
    "# # Define query\n",
    "# query7 = \"(#globalwarming OR 'global warming') lang:en -filter:retweets since:2022-10-01 until:2023-11-01\"\n",
    "\n",
    "# # Call the function to scrape tweets\n",
    "# tweets7 = scrape_tweets(query7)\n",
    "\n",
    "# # Save the final scraped data\n",
    "# save_data(tweets7)\n",
    "# print(\"Data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f528c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.read_csv(\"global warming tweets_7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58caa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b327668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the file path for saving the data\n",
    "# file_path = \"global warming tweets_8.csv\"\n",
    "\n",
    "# # Define query\n",
    "# query8 = \"(#globalwarming OR 'global warming') lang:en -filter:retweets since:2022-09-01 until:2023-10-01\"\n",
    "\n",
    "# # Call the function to scrape tweets\n",
    "# tweets8 = scrape_tweets(query8)\n",
    "\n",
    "# # Save the final scraped data\n",
    "# save_data(tweets8)\n",
    "# print(\"Data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5287fca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
